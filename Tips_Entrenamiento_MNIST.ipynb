{
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalvarezme/AprendizajeMaquina/blob/main/7_TopicosAvanzados/2_Autoencoders/3_Autoencoder_TwoOutputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Sequence\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-12T00:20:52.692548Z",
          "iopub.execute_input": "2024-03-12T00:20:52.692980Z",
          "iopub.status.idle": "2024-03-12T00:20:52.699817Z",
          "shell.execute_reply.started": "2024-03-12T00:20:52.692944Z",
          "shell.execute_reply": "2024-03-12T00:20:52.698792Z"
        },
        "trusted": true,
        "id": "5L83I57xvR_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale: float = 0.4 #variabilidad del ruido\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() #cargo la base de datos mnist\n",
        "x_train = x_train.astype('float32') / 255. + np.random.normal(scale=scale, size=x_train.shape) #normalizo la imagen y agrego ruido\n",
        "x_test = x_test.astype('float32') / 255. + np.random.normal(scale=scale, size=x_test.shape) #normalizo la imagen y agrego ruido\n",
        "\n",
        "# Creo conjuntos de entrenamiento, validación y prueba\n",
        "x_val = x_train[50000:] # para validar, desde el dato 50.000 en adelante, es decir, 10.000 datos para validar\n",
        "y_val = y_train[50000:] # para validar, desde el dato 50.000 en adelante, es decir, 10.000 datos para validar\n",
        "x_train = x_train[:50000] # para entrenar los primeros 50.000 datos\n",
        "x_train = x_train[..., tf.newaxis] # nueva dimensión para los filtros convolucionales\n",
        "y_train = y_train[:50000] # entrenar los primeros 50.000 datos\n",
        "x_val = x_val[..., tf.newaxis] # nueva dimensión para los filtros convolucionales\n",
        "x_val = x_val[..., tf.newaxis] # nueva dimensión para los filtros convolucionales\n",
        "x_test = x_test[..., tf.newaxis] # nueva dimensión para los filtros convolucionales\n"
      ],
      "metadata": {
        "id": "ArFhHj3KY3qO",
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:24.476642Z",
          "iopub.execute_input": "2024-03-11T23:44:24.477407Z",
          "iopub.status.idle": "2024-03-11T23:44:26.689152Z",
          "shell.execute_reply.started": "2024-03-11T23:44:24.477369Z",
          "shell.execute_reply": "2024-03-11T23:44:26.688008Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape,x_val.shape,x_test.shape,y_train.shape,y_val.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "Ci4jpyZ8ZxER",
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:26.690482Z",
          "iopub.execute_input": "2024-03-11T23:44:26.690806Z",
          "iopub.status.idle": "2024-03-11T23:44:26.696615Z",
          "shell.execute_reply.started": "2024-03-11T23:44:26.690771Z",
          "shell.execute_reply": "2024-03-11T23:44:26.695534Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mnist_autoencoder(\n",
        "    x: Sequence[np.ndarray],\n",
        "    x_: Sequence[np.ndarray],\n",
        "    y: Optional[Sequence] = None,\n",
        "    y_: Optional[Sequence] = None,\n",
        "    cmap: str = 'gray',\n",
        "    vmin: float = 0,\n",
        "    vmax: float = 1\n",
        ") -> None:\n",
        "\n",
        "    plt.figure(figsize=(20, 2))\n",
        "    for i, (train, predict) in enumerate(zip(x, x_), start=1):\n",
        "        plt.subplot(2, len(x), i)\n",
        "        plt.imshow(train.reshape(28, 28), cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, len(x), i + len(x))\n",
        "        plt.imshow(predict.reshape(28, 28), cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        plt.axis('off')\n",
        "\n",
        "        if y is not None and y_ is not None:\n",
        "            color = \"red\" if y[i - 1] != y_[i - 1] else 'green'\n",
        "            plt.text(\n",
        "                0, 28, str(y_[i - 1]), color=color, fontsize=15,\n",
        "                verticalalignment='bottom', horizontalalignment='left'\n",
        "            )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:28.219526Z",
          "iopub.execute_input": "2024-03-11T23:44:28.219848Z",
          "iopub.status.idle": "2024-03-11T23:44:28.231926Z",
          "shell.execute_reply.started": "2024-03-11T23:44:28.219804Z",
          "shell.execute_reply": "2024-03-11T23:44:28.230875Z"
        },
        "trusted": true,
        "id": "xpdb2fFtvR_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot images on latent space\n",
        "def plot_mnist_2d(Z,y,images,img_w=28,img_h=28,zoom=0.5,cmap='jet'):\n",
        "    fig, ax = plt.subplots(figsize=(5,5))\n",
        "    plt.axis('off')\n",
        "    for i in range(Z.shape[0]):\n",
        "        #print('img',i+1,'/',Z.shape[0])\n",
        "        image = images[i].reshape((img_w, img_h))\n",
        "        im = OffsetImage(image, zoom=zoom,cmap=cmap)\n",
        "        ab = AnnotationBbox(im, (Z[i,0], Z[i,1]), xycoords='data', frameon=False)\n",
        "        ax.add_artist(ab)\n",
        "        ax.update_datalim([(Z[i,0], Z[i,1])])\n",
        "        ax.autoscale()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "N7-0eUO8Z5jD",
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:28.233221Z",
          "iopub.execute_input": "2024-03-11T23:44:28.233517Z",
          "iopub.status.idle": "2024-03-11T23:44:28.244149Z",
          "shell.execute_reply.started": "2024-03-11T23:44:28.233493Z",
          "shell.execute_reply": "2024-03-11T23:44:28.243234Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:28.245443Z",
          "iopub.execute_input": "2024-03-11T23:44:28.246248Z",
          "iopub.status.idle": "2024-03-11T23:44:28.257648Z",
          "shell.execute_reply.started": "2024-03-11T23:44:28.246214Z",
          "shell.execute_reply": "2024-03-11T23:44:28.256667Z"
        },
        "trusted": true,
        "id": "5_S-u-IqvR_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.reshape(x_train.shape[0],-1).shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:28.258940Z",
          "iopub.execute_input": "2024-03-11T23:44:28.259918Z",
          "iopub.status.idle": "2024-03-11T23:44:28.267133Z",
          "shell.execute_reply.started": "2024-03-11T23:44:28.259879Z",
          "shell.execute_reply": "2024-03-11T23:44:28.266210Z"
        },
        "trusted": true,
        "id": "v8_KBdJ0vR_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#traditional PCA algorithm\n",
        "red = PCA(n_components=2, random_state=123) #encontrar componentes principales que  ayuden a maximizar la variabilidad de los datos\n",
        "Z = red.fit_transform(x_train.reshape(x_train.shape[0],-1)) #entrenar, cambiando las dimensiones\n",
        "N = 500 #  500 imagenes  espacio latente\n",
        "plot_mnist_2d(Z[:N],y_train[:N],x_train[:N],img_w=28,img_h=28,zoom=0.3,cmap='gray')"
      ],
      "metadata": {
        "id": "rRKQyjxAZ8CL",
        "execution": {
          "iopub.status.busy": "2024-03-11T23:49:27.940921Z",
          "iopub.execute_input": "2024-03-11T23:49:27.941308Z",
          "iopub.status.idle": "2024-03-11T23:49:32.790753Z",
          "shell.execute_reply.started": "2024-03-11T23:49:27.941277Z",
          "shell.execute_reply": "2024-03-11T23:49:32.789784Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el objeto de pérdida y el optimizador\n",
        "tf.keras.backend.clear_session() #limpiar la memoria\n",
        "\n",
        "# Definir el modelo autoencoder\n",
        "input_img = Input(shape=(28, 28, 1)) #defino el tamaño de la entrada\n",
        "\n",
        "# Encoder\n",
        "\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img) #convolución 2D (16 filtros, 3x3 es el tamaño del kernel, sobre la imagen de entrada)\n",
        "xe = Conv2D(8, (3, 3), activation='relu', padding='same')(x) #convolución 2D (8 filtros, 3x3 es el tamaño del kernel, sobre la salida de la convolución anterior)\n",
        "\n",
        "# Decoder\n",
        "\n",
        "x = Conv2DTranspose(8, (3, 3), activation='relu', padding='same')(xe) #desconvolución del espacio latente de la capa anterior (reflejo)\n",
        "x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x) #desconvolución del espacio latente de la capa anterior (reflejo)\n",
        "reconstructed_img = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) #se reconstruye la imagen con activación sigmoide para normalizar\n",
        "\n",
        "# Rama de clasificación\n",
        "\n",
        "x = Flatten(name='fencoded')(xe) #aplico flatten al espacio latente\n",
        "classification_output = Dense(10, activation='softmax')(x) #salida de clasificación con activación softmax para obtener probabilidades, 10 clases\n",
        "\n",
        "# Definir el modelo con dos salidas\n",
        "\n",
        "autoencoder = Model(inputs=input_img, outputs=[reconstructed_img, classification_output]) #describe las entradas y salidas del modelo, en la API Funcional de Keras\n"
      ],
      "metadata": {
        "id": "q_S_lr4QZmjJ",
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:33.239432Z",
          "iopub.execute_input": "2024-03-11T23:44:33.239819Z",
          "iopub.status.idle": "2024-03-11T23:44:34.366068Z",
          "shell.execute_reply.started": "2024-03-11T23:44:33.239785Z",
          "shell.execute_reply": "2024-03-11T23:44:34.365045Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom loss function\n",
        "\n",
        "def custom_loss(lambda_=0.5):\n",
        "    def custom_loss_autoencoder(y_true, y_pred):\n",
        "        reconstruction_loss = MeanSquaredError()(y_true[0], y_pred[0]) #con mse (error cuadrático medio)para minimizar la distancia\n",
        "        classification_loss = SparseCategoricalCrossentropy()(y_true[1], y_pred[1]) #pseudositancia que minimiza la distancia entre las función de densidad de probabilidad (softmax)\n",
        "        return lambda_*reconstruction_loss + (1-lambda_)*classification_loss # va a retornar la suma de ambos loss (sopésada por lambda)\n",
        "    return custom_loss_autoencoder #retorna la función\n",
        "\n",
        "lam_ = 0.25 #peso del error de la reconstrucción\n",
        "autoencoder.compile(optimizer=Adam(), loss=custom_loss(lambda_=lam_)) #compila el modelo con optimizador Adam y función de costo"
      ],
      "metadata": {
        "id": "monQuofBbTTe",
        "execution": {
          "iopub.status.busy": "2024-03-11T23:44:34.367625Z",
          "iopub.execute_input": "2024-03-11T23:44:34.368150Z",
          "iopub.status.idle": "2024-03-11T23:44:34.385763Z",
          "shell.execute_reply.started": "2024-03-11T23:44:34.368113Z",
          "shell.execute_reply": "2024-03-11T23:44:34.384792Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom training loop\n",
        "batch_size = 64 #cantidad de imagenes de una iteración\n",
        "epochs = 20\n",
        "N = 500 #numero de imagenes que vamos a plotear\n",
        "red = PCA(n_components=2, random_state=123) #definir PCA\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    for x_batch, y_batch in tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024).batch(batch_size): #para cada lote ...\n",
        "        with tf.GradientTape() as tape: # gradiente\n",
        "            reconstruction, classification = autoencoder(x_batch, training=True) #sacar la resconstruccion y clasificación\n",
        "            loss = autoencoder.loss([x_batch, y_batch], [reconstruction, classification]) #calcular el loss del lote\n",
        "            gradients = tape.gradient(loss, autoencoder.trainable_variables) #calcular el gradiente\n",
        "        autoencoder.optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables)) #aplicar el gradiente\n",
        "\n",
        "    loss_ = [] #lista de métricas\n",
        "    for x_val_batch, y_val_batch in tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(buffer_size=128).batch(batch_size): #para cada lote de validación\n",
        "        val_reconstruction, val_classification = autoencoder(x_val_batch, training=False) #calcular la resconstrucción y la clasificación\n",
        "        loss_.append(autoencoder.loss([x_val_batch, y_val_batch], [val_reconstruction, val_classification])) #calculamos el loss y agrgar a la lista de métricas\n",
        "    print(f'Loss: {loss.numpy()} Val_loss: {np.array(loss_).mean()}') #imprimimos el loss\n",
        "    if (epoch+1)%5 == 0: #si la epoca es multiplo de 5...\n",
        "\n",
        "      encoder_ = tf.keras.Model(inputs=autoencoder.inputs,outputs=autoencoder.get_layer('fencoded').output)\n",
        "      Z = red.fit_transform(encoder_(x_val))\n",
        "\n",
        "      #plot_mnist_2d(Z[:N],y_val[:N],x_val[:N],img_w=28,img_h=28,zoom=0.3,cmap='gray')\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "Ad5tqyHacAvy",
        "execution": {
          "iopub.status.busy": "2024-03-11T23:59:17.408760Z",
          "iopub.execute_input": "2024-03-11T23:59:17.409617Z",
          "iopub.status.idle": "2024-03-12T00:20:52.690418Z",
          "shell.execute_reply.started": "2024-03-11T23:59:17.409584Z",
          "shell.execute_reply": "2024-03-12T00:20:52.689284Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar predicciones con el modelo\n",
        "reconstruccion_test, clasificacion_test = autoencoder.predict(x_test) #realizo la predicción del modelo en los datos de testeo\n",
        "\n",
        "# Calcular el Error Cuadrático Medio para la reconstrucción\n",
        "mse_reconstruction = MeanSquaredError()(x_test, reconstruccion_test) #calculo el error de reconstrucción de los datos de testeo\n",
        "\n",
        "# Preparar las etiquetas verdaderas y predichas para calcular el puntaje F1\n",
        "# Convertir etiquetas de formato categórico (one-hot) a formato de clase única\n",
        "\n",
        "y_test_classes = np.argmax(y_test.reshape(-1, 1), axis=1) #convierto one-hot a sparse\n",
        "y_pca_pred_classes = np.argmax(clasificacion_test, axis=1) #convierto one-hot a sparse\n",
        "\n",
        "# Calcular el puntaje F1 para la clasificación\n",
        "f1 = f1_score(y_test_classes, y_pca_pred_classes, average='weighted') #calculo el puntaje F1\n",
        "\n",
        "print(f\"Error Cuadrático Medio (Reconstrucción): {mse_reconstruction}\") #imprimo el error de reconstrucción\n",
        "print(f\"Puntaje F1 (Clasificación): {f1}\") #imprimo el puntaje F1 de clasificación\n"
      ],
      "metadata": {
        "id": "ZczebgpFeR38",
        "execution": {
          "iopub.status.busy": "2024-03-12T00:30:18.127475Z",
          "iopub.execute_input": "2024-03-12T00:30:18.128159Z",
          "iopub.status.idle": "2024-03-12T00:30:19.054647Z",
          "shell.execute_reply.started": "2024-03-12T00:30:18.128124Z",
          "shell.execute_reply": "2024-03-12T00:30:19.053616Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of images to display\n",
        "N = 20\n",
        "\n",
        "plot_mnist_autoencoder(\n",
        "    x_test[:N],  # First N original test images\n",
        "    reconstruccion_test[:N],   # First N reconstructed images\n",
        "    y_test[:N],  # True labels for the first N images\n",
        "    clasificacion_test.argmax(axis=1)[:N]    # Predicted labels for the first N images\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-12T00:31:09.881741Z",
          "iopub.execute_input": "2024-03-12T00:31:09.882781Z",
          "iopub.status.idle": "2024-03-12T00:31:11.832957Z",
          "shell.execute_reply.started": "2024-03-12T00:31:09.882748Z",
          "shell.execute_reply": "2024-03-12T00:31:11.831860Z"
        },
        "trusted": true,
        "id": "UXXdsoHbvR_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 300"
      ],
      "metadata": {
        "id": "y-3EthIR3uq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mnist_2d(Z[:N], y_val[:N], x_val[:N], img_w=28, img_h=28, zoom=0.3, cmap='gray')\n"
      ],
      "metadata": {
        "id": "KhNRUcU53jIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}